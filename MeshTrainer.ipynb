{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T16:31:09.323479Z","iopub.status.busy":"2024-06-06T16:31:09.322599Z","iopub.status.idle":"2024-06-06T16:31:30.718560Z","shell.execute_reply":"2024-06-06T16:31:30.717443Z","shell.execute_reply.started":"2024-06-06T16:31:09.323408Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n","  Cloning https://github.com/MarcusLoppe/meshgpt-pytorch.git to /tmp/pip-req-build-wddw5nk8\n","  Running command git clone --filter=blob:none --quiet https://github.com/MarcusLoppe/meshgpt-pytorch.git /tmp/pip-req-build-wddw5nk8\n","  Resolved https://github.com/MarcusLoppe/meshgpt-pytorch.git to commit c00e7c6bd40f1cd56c369f598ad357c591cd12e1\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: accelerate>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (0.30.1)\n","Requirement already satisfied: beartype in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (0.18.5)\n","Requirement already satisfied: classifier-free-guidance-pytorch>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (0.6.2)\n","Requirement already satisfied: einops>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (0.8.0)\n","Requirement already satisfied: einx[torch]>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (0.2.2)\n","Requirement already satisfied: ema-pytorch in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (0.4.8)\n","Requirement already satisfied: local-attention>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (1.9.1)\n","Requirement already satisfied: gateloop-transformer>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (0.2.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (1.26.4)\n","Requirement already satisfied: pytorch-custom-utils>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (0.0.19)\n","Requirement already satisfied: taylor-series-linear-attention>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (0.1.9)\n","Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (2.3.1)\n","Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (2.5.3)\n","Requirement already satisfied: torchtyping in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (0.1.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (4.66.4)\n","Requirement already satisfied: vector-quantize-pytorch>=1.14.22 in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (1.14.24)\n","Requirement already satisfied: x-transformers>=1.30.6 in /usr/local/lib/python3.10/dist-packages (from meshgpt-pytorch==1.2.18) (1.30.11)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.25.0->meshgpt-pytorch==1.2.18) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.25.0->meshgpt-pytorch==1.2.18) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.25.0->meshgpt-pytorch==1.2.18) (6.0)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.25.0->meshgpt-pytorch==1.2.18) (0.23.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.25.0->meshgpt-pytorch==1.2.18) (0.4.3)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch>=0.6.2->meshgpt-pytorch==1.2.18) (6.2.0)\n","Requirement already satisfied: open-clip-torch>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch>=0.6.2->meshgpt-pytorch==1.2.18) (2.24.0)\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch>=0.6.2->meshgpt-pytorch==1.2.18) (4.41.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from einx[torch]>=0.1.3->meshgpt-pytorch==1.2.18) (1.12.1)\n","Requirement already satisfied: frozendict in /usr/local/lib/python3.10/dist-packages (from einx[torch]>=0.1.3->meshgpt-pytorch==1.2.18) (2.4.4)\n","Requirement already satisfied: rotary-embedding-torch in /usr/local/lib/python3.10/dist-packages (from gateloop-transformer>=0.2.2->meshgpt-pytorch==1.2.18) (0.6.2)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from pytorch-custom-utils>=0.0.9->meshgpt-pytorch==1.2.18) (0.11.0)\n","Requirement already satisfied: pytorch-warmup in /usr/local/lib/python3.10/dist-packages (from pytorch-custom-utils>=0.0.9->meshgpt-pytorch==1.2.18) (0.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (4.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (2024.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->meshgpt-pytorch==1.2.18) (2.3.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1->meshgpt-pytorch==1.2.18) (12.5.40)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric->meshgpt-pytorch==1.2.18) (1.13.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric->meshgpt-pytorch==1.2.18) (3.9.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric->meshgpt-pytorch==1.2.18) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric->meshgpt-pytorch==1.2.18) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric->meshgpt-pytorch==1.2.18) (1.5.0)\n","Requirement already satisfied: typeguard>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from torchtyping->meshgpt-pytorch==1.2.18) (4.3.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.6.2->meshgpt-pytorch==1.2.18) (0.18.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.6.2->meshgpt-pytorch==1.2.18) (2024.5.15)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.6.2->meshgpt-pytorch==1.2.18) (0.2.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.6.2->meshgpt-pytorch==1.2.18) (5.27.1)\n","Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.6.2->meshgpt-pytorch==1.2.18) (1.0.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.2.18) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.2.18) (23.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.2.18) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.2.18) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.2.18) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->meshgpt-pytorch==1.2.18) (4.0.3)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->classifier-free-guidance-pytorch>=0.6.2->meshgpt-pytorch==1.2.18) (0.2.13)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1->meshgpt-pytorch==1.2.18) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->meshgpt-pytorch==1.2.18) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torch_geometric->meshgpt-pytorch==1.2.18) (2.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torch_geometric->meshgpt-pytorch==1.2.18) (1.25.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torch_geometric->meshgpt-pytorch==1.2.18) (2019.11.28)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric->meshgpt-pytorch==1.2.18) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric->meshgpt-pytorch==1.2.18) (3.5.0)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->einx[torch]>=0.1.3->meshgpt-pytorch==1.2.18) (1.3.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch>=0.6.2->meshgpt-pytorch==1.2.18) (0.19.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch>=2.8.0->classifier-free-guidance-pytorch>=0.6.2->meshgpt-pytorch==1.2.18) (10.3.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.9.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n","/root/text_to_mesh\n"]}],"source":["!pip install git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n","!pip install matplotlib\n","%cd /root/text_to_mesh"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T16:31:36.285258Z","iopub.status.busy":"2024-06-06T16:31:36.284932Z","iopub.status.idle":"2024-06-06T16:31:45.066082Z","shell.execute_reply":"2024-06-06T16:31:45.065241Z","shell.execute_reply.started":"2024-06-06T16:31:36.285228Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from pathlib import Path \n","import gc    \n","import torch\n","import os\n","import torch  \n","from meshgpt_pytorch import (\n","    MeshTransformerTrainer,\n","    MeshAutoencoderTrainer,\n","    MeshAutoencoder,\n","    MeshTransformer,MeshDataset\n",")\n","from meshgpt_pytorch.data import ( \n","    derive_face_edges_from_faces\n",")   \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T16:31:45.081859Z","iopub.status.busy":"2024-06-06T16:31:45.081213Z","iopub.status.idle":"2024-06-06T16:31:46.271695Z","shell.execute_reply":"2024-06-06T16:31:46.270609Z","shell.execute_reply.started":"2024-06-06T16:31:45.081824Z"},"trusted":true},"outputs":[],"source":["autoencoder = MeshAutoencoder( \n","        decoder_dims_through_depth =  (128,) * 6 + (192,) * 12 + (256,) * 24 + (384,) * 6,   \n","        codebook_size =  2048, \n","        dim_codebook = 192,  \n","        dim_area_embed = 16,\n","        dim_coor_embed = 16, \n","        dim_normal_embed = 16,\n","        dim_angle_embed = 8,\n","    \n","    attn_decoder_depth  = 4,\n","    attn_encoder_depth = 2\n","    ).to(\"cuda\")     "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["\n","torch.cuda.empty_cache()\n","gc.collect()  \n"," \n","transformer = MeshTransformer(\n","    autoencoder,\n","    dim = 768,\n","    coarse_pre_gateloop_depth =2,  \n","    fine_pre_gateloop_depth= 2, \n","    attn_depth = 12,  \n","    attn_heads = 12, \n","    fine_cross_attend_text = True,\n","    text_cond_with_film = False,\n","    cross_attn_num_mem_kv = 4,\n","    num_sos_tokens = 1, \n","    dropout  = 0.0,\n","    max_seq_len = 1500, \n","    fine_attn_depth = 2,\n","    condition_on_text = True, \n","    gateloop_use_heinsen = False,\n","    text_condition_model_types = \"bge\", \n","    text_condition_cond_drop_prob = 0.25, \n",").to(\"cuda\")\n","pkg = torch.load(\"./MeshGPT-transformer_trained_01.pt\") \n","transformer.load_state_dict(pkg['model'],strict=False)\n"," "]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T16:43:43.560111Z","iopub.status.busy":"2024-06-06T16:43:43.559376Z","iopub.status.idle":"2024-06-06T16:43:51.699031Z","shell.execute_reply":"2024-06-06T16:43:51.698098Z","shell.execute_reply.started":"2024-06-06T16:43:43.560073Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[MeshDataset] Loaded 15170 entrys\n","[MeshDataset] Created from 15170 entrys\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 102/102 [00:06<00:00, 15.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["[MeshDataset] Generated codes for 15170 entrys\n","[MeshDataset] Generated 885 text_embeddings\n"]}],"source":["dataset = MeshDataset.load(\"./labels_885_10x5_21720_mod.npz\") \n","labels = list(set(item['texts'] for item in dataset.data))\n","dataset.generate_codes(autoencoder,150)\n","dataset.data[0].keys() \n","dataset.embed_texts(transformer,1)\n"," "]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/740:   0%|          | 1/948 [00:00<03:04,  5.12it/s, loss=0.49]"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/740:   6%|▌         | 58/948 [00:13<03:33,  4.16it/s, loss=1.05]  \n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 254.00 MiB. GPU ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m grad_accum_every \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m      \n\u001b[1;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MeshTransformerTrainer(model \u001b[38;5;241m=\u001b[39m transformer,warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,grad_accum_every\u001b[38;5;241m=\u001b[39mgrad_accum_every,num_train_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, dataset \u001b[38;5;241m=\u001b[39m dataset, \n\u001b[1;32m      7\u001b[0m                                  learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size ,checkpoint_every_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m) \n\u001b[0;32m----> 8\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m740\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_at_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00005\u001b[39;49m\u001b[43m)\u001b[49m   \n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/meshgpt_pytorch/trainer.py:679\u001b[0m, in \u001b[0;36mMeshTransformerTrainer.train\u001b[0;34m(self, num_epochs, stop_at_loss, diplay_graph)\u001b[0m\n\u001b[1;32m    676\u001b[0m maybe_no_sync \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mautocast(), maybe_no_sync(): \n\u001b[0;32m--> 679\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_accum_every)\n\u001b[1;32m    682\u001b[0m current_loss \u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/meshgpt_pytorch/meshgpt_pytorch.py:1351\u001b[0m, in \u001b[0;36mMeshTransformer.forward\u001b[0;34m(self, vertices, faces, face_edges, codes, cache, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists(codes):\n\u001b[1;32m   1345\u001b[0m     codes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder\u001b[38;5;241m.\u001b[39mtokenize(\n\u001b[1;32m   1346\u001b[0m         vertices \u001b[38;5;241m=\u001b[39m vertices,\n\u001b[1;32m   1347\u001b[0m         faces \u001b[38;5;241m=\u001b[39m faces,\n\u001b[1;32m   1348\u001b[0m         face_edges \u001b[38;5;241m=\u001b[39m face_edges\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[0;32m-> 1351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_on_codes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/classifier_free_guidance_pytorch/classifier_free_guidance_pytorch.py:141\u001b[0m, in \u001b[0;36mclassifier_free_guidance.<locals>.inner\u001b[0;34m(self, cond_scale, rescale_phi, cfg_routed_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m cond_scale \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myou cannot do condition scaling when in training mode\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn_maybe_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m cond_scale \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid conditioning scale, must be greater or equal to 1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    145\u001b[0m kwargs_without_cond_dropout \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, cond_drop_prob_keyname: \u001b[38;5;241m0.\u001b[39m}\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/classifier_free_guidance_pytorch/classifier_free_guidance_pytorch.py:134\u001b[0m, in \u001b[0;36mclassifier_free_guidance.<locals>.inner.<locals>.fn_maybe_with_text\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_text_cond\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m fn_params:\n\u001b[1;32m    132\u001b[0m         kwargs\u001b[38;5;241m.\u001b[39mupdate(raw_text_cond \u001b[38;5;241m=\u001b[39m raw_text_cond)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/meshgpt_pytorch/meshgpt_pytorch.py:1606\u001b[0m, in \u001b[0;36mMeshTransformer.forward_on_codes\u001b[0;34m(self, codes, return_loss, return_cache, append_eos, cache, texts, text_embeds, cond_drop_prob)\u001b[0m\n\u001b[1;32m   1602\u001b[0m     fine_vertex_codes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_fine_film_cond(fine_vertex_codes, pooled_text_embed)\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;66;03m# fine transformer\u001b[39;00m\n\u001b[0;32m-> 1606\u001b[0m attended_vertex_codes, fine_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfine_decoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfine_vertex_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfine_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfine_attn_context_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_hiddens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   1611\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_cache_fine:\n\u001b[1;32m   1614\u001b[0m     fine_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/x_transformers/x_transformers.py:1495\u001b[0m, in \u001b[0;36mAttentionLayers.forward\u001b[0;34m(self, x, context, mask, context_mask, attn_mask, self_attn_kv_mask, mems, mem_masks, seq_start_pos, cache, cache_age, return_hiddens, rotary_pos_emb)\u001b[0m\n\u001b[1;32m   1493\u001b[0m     out, inter \u001b[38;5;241m=\u001b[39m block(x, context \u001b[38;5;241m=\u001b[39m context, mask \u001b[38;5;241m=\u001b[39m mask, context_mask \u001b[38;5;241m=\u001b[39m context_mask, prev_attn \u001b[38;5;241m=\u001b[39m prev_cross_attn, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iter_attn_cache, \u001b[38;5;28;01mNone\u001b[39;00m), return_intermediates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m layer_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1495\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresi_dual:\n\u001b[1;32m   1498\u001b[0m     outer_residual \u001b[38;5;241m=\u001b[39m outer_residual \u001b[38;5;241m+\u001b[39m out \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresi_dual_scale\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/x_transformers/x_transformers.py:749\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/x_transformers/x_transformers.py:701\u001b[0m, in \u001b[0;36mGLU.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    700\u001b[0m     x, gate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmult_bias\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 254.00 MiB. GPU "]}],"source":[" \n","torch.cuda.empty_cache()\n","gc.collect()  \n"," \n","batch_size =16\n","grad_accum_every =4      \n","trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=grad_accum_every,num_train_steps=100, dataset = dataset, \n","                                 learning_rate = 1e-4, batch_size=batch_size ,checkpoint_every_epoch = 25) \n","loss = trainer.train(740, stop_at_loss = 0.00005)   "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pkg = dict( model = transformer.state_dict(), ) \n","torch.save(pkg, str(\"./MeshGPT-transformer_trained.pt\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Generating  box\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 31/1500 [00:00<00:04, 295.15it/s]"]},{"name":"stderr","output_type":"stream","text":[" 21%|██        | 312/1500 [00:01<00:03, 311.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Generating  hose\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 1002/1500 [00:03<00:01, 313.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Generating  rake\n"]},{"name":"stderr","output_type":"stream","text":[" 53%|█████▎    | 792/1500 [00:02<00:02, 313.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Generating  shovel\n"]},{"name":"stderr","output_type":"stream","text":[" 34%|███▎      | 504/1500 [00:01<00:03, 313.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Generating  cart\n"]},{"name":"stderr","output_type":"stream","text":[" 82%|████████▏ | 1236/1500 [00:03<00:00, 315.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Generating  bin\n"]},{"name":"stderr","output_type":"stream","text":[" 62%|██████▏   | 936/1500 [00:02<00:01, 313.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Generating  trash can\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|███       | 456/1500 [00:01<00:03, 318.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Generating  ladder\n"]},{"name":"stderr","output_type":"stream","text":[" 42%|████▏     | 624/1500 [00:01<00:02, 313.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Generating  chair\n"]},{"name":"stderr","output_type":"stream","text":[" 43%|████▎     | 648/1500 [00:02<00:02, 313.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Generating  table\n"]},{"name":"stderr","output_type":"stream","text":[" 58%|█████▊    | 864/1500 [00:02<00:02, 312.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Generating  bed\n"]},{"name":"stderr","output_type":"stream","text":[" 85%|████████▍ | 1272/1500 [00:04<00:00, 313.31it/s]\n"]}],"source":["from meshgpt_pytorch import mesh_render\n","text_coords = []\n","rows = []\n","transformer.eval() \n","\n","for text in [\"box\", \"hose\" ,\"rake\" ,\"shovel\", \"cart\", \"bin\",\"trash can\" ,\"ladder\" ,\"chair\", \"table\",  \"bed\"] :\n","    print(\"Generating \", text) \n","    face_coords = transformer.generate(texts = [text] ,  temperature = 0.0)\n","    text_coords.append(face_coords)\n","\n","rows.append(text_coords)\n","mesh_render.combind_mesh_with_rows(f'./rows.obj', rows)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5157660,"sourceId":8617083,"sourceType":"datasetVersion"},{"datasetId":5157661,"sourceId":8617085,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
