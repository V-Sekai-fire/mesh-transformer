{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9bbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baeb840",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "import gc    \n",
    "import torch\n",
    "import os\n",
    "import torch  \n",
    "from meshgpt_pytorch import (\n",
    "    MeshTransformerTrainer,\n",
    "    MeshAutoencoderTrainer,\n",
    "    MeshAutoencoder,\n",
    "    MeshTransformer,MeshDataset\n",
    ")\n",
    "from meshgpt_pytorch.data import ( \n",
    "    derive_face_edges_from_faces\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /root/text-to-mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = MeshAutoencoder( \n",
    "        decoder_dims_through_depth =  (128,) * 6 + (192,) * 12 + (256,) * 24 + (384,) * 6,    \n",
    "        dim_codebook = 192,  \n",
    "        dim_area_embed = 16,\n",
    "        dim_coor_embed = 16, \n",
    "        dim_normal_embed = 16,\n",
    "        dim_angle_embed = 8,\n",
    "    \n",
    "    attn_decoder_depth  = 4,\n",
    "    attn_encoder_depth = 2\n",
    "    ).to(\"cuda\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ecc8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MeshDataset.load(\"./objverse_250f_45.9M_3086_labels_53730_10_min_x1_aug.npz\")  \n",
    "dataset2 = MeshDataset.load(\"./objverse_250f_229.7M_3086_labels_268650_10_min_x5_aug.npz\")\n",
    "dataset.data.extend(dataset2.data)  \n",
    "dataset2 = MeshDataset.load(\"./shapenet_250f_2.2M_84_labels_2156_10_min_x1_aug.npz\")  \n",
    "dataset.data.extend(dataset2.data)  \n",
    "dataset2 = MeshDataset.load(\"./shapenet_250f_21.9M_84_labels_21560_10_min_x10_aug.npz\")  \n",
    "dataset.data.extend(dataset2.data) \n",
    "dataset.sort_dataset_keys()\n",
    "print(\"length\", len(dataset.data))\n",
    "\n",
    "def format_value(value):\n",
    "    if value >= 1_000_000_000:\n",
    "        return f\"{value / 1_000_000_000:.1f}B\"\n",
    "    elif value >= 1_000_000:\n",
    "        return f\"{value / 1_000_000:.1f}M\"\n",
    "    else:\n",
    "        return f\"{value}\"\n",
    "\n",
    "tokens = 0\n",
    "for item in dataset.data:\n",
    "    tokens += len(item['faces']) * 6 \n",
    "total_tokens = format_value(tokens)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkg = torch.load(\"checkpoints/mesh-autoencoder.ckpt.epoch_5_avg_loss_0.17483_recon_0.3439_commit_-0.6763.pt\") \n",
    "# autoencoder.load_state_dict(pkg['model'])\n",
    "# for param in autoencoder.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad6ef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16 # The batch size should be max 64.\n",
    "grad_accum_every =4\n",
    "# So set the maximal batch size (max 64) that your VRAM can handle and then use grad_accum_every to create a effective batch size of 64, e.g  16 * 4 = 64\n",
    "learning_rate = 1e-3 # Start with 1e-3 then at stagnation around 0.35, you can lower it to 1e-4.\n",
    "\n",
    "autoencoder.commit_loss_weight = 0.2 # Set dependant on the dataset size, on smaller datasets, 0.1 is fine, otherwise try from 0.25 to 0.4.\n",
    "autoencoder_trainer = MeshAutoencoderTrainer(model =autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100,\n",
    "                                             batch_size=batch_size,\n",
    "                                             grad_accum_every = grad_accum_every,\n",
    "                                             learning_rate = learning_rate,\n",
    "                                             checkpoint_every_epoch=5)\n",
    " \n",
    "loss = autoencoder_trainer.train(480, diplay_graph= True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2cc14",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "now_utc = datetime.datetime.now(datetime.timezone.utc).isoformat()\n",
    "now_safe = now_utc.replace(\":\", \"_\")\n",
    "pkg = dict( model = autoencoder.state_dict(), )\n",
    "filename = f\"./MeshGPT-autoencoder_{now_safe}.pt\"\n",
    "torch.save(pkg, filename)\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "from meshgpt_pytorch import mesh_render \n",
    "\n",
    "min_mse, max_mse = float('inf'), float('-inf')\n",
    "min_coords, min_orgs, max_coords, max_orgs = None, None, None, None\n",
    "random_samples, random_samples_pred, all_random_samples = [], [], []\n",
    "total_mse, sample_size = 0.0, 200\n",
    "\n",
    "random.shuffle(dataset.data)\n",
    "autoencoder = autoencoder.to(\"cuda\")\n",
    "for item in tqdm(dataset.data[:sample_size]):\n",
    "    item['faces'] = item['faces'].to(\"cuda\")\n",
    "    item['vertices'] = item['vertices'].to(\"cuda\")\n",
    "    item['face_edges'] = item['face_edges'].to(\"cuda\")\n",
    "    codes = autoencoder.tokenize(vertices=item['vertices'], faces=item['faces'], face_edges=item['face_edges']) \n",
    "    \n",
    "    codes = codes.flatten().unsqueeze(0)\n",
    "    codes = codes[:, :codes.shape[-1] // autoencoder.num_quantizers * autoencoder.num_quantizers] \n",
    " \n",
    "    coords, mask = autoencoder.decode_from_codes_to_faces(codes)\n",
    "    orgs = item['vertices'][item['faces']].unsqueeze(0)\n",
    "\n",
    "    mse = torch.mean((orgs.view(-1, 3).cpu() - coords.view(-1, 3).cpu())**2)\n",
    "    total_mse += mse \n",
    "\n",
    "    if mse < min_mse: min_mse, min_coords, min_orgs = mse, coords, orgs\n",
    "    if mse > max_mse: max_mse, max_coords, max_orgs = mse, coords, orgs\n",
    " \n",
    "    if len(random_samples) <= 30:\n",
    "        random_samples.append((coords, mask)) \n",
    "    else:\n",
    "        all_random_samples.extend([ random_samples])\n",
    "        random_samples, random_samples_pred = [], []\n",
    "\n",
    "print(f'MSE AVG: {total_mse / sample_size:.10f}, Min: {min_mse:.10f}, Max: {max_mse:.10f}')    \n",
    "mesh_render.save_rendering(f'./mse_rows.obj', all_random_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.conditioner.text_models[0].model.to(\"cuda\")\n",
    "transformer = transformer.to(\"cuda\")\n",
    "\n",
    "dataset.embed_texts(transformer,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf004b7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    " \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()  \n",
    " \n",
    "batch_size = 2\n",
    "grad_accum_every =4     \n",
    "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=grad_accum_every,num_train_steps=100, dataset = dataset, \n",
    "                                  accelerator_kwargs = {\"mixed_precision\" : \"fp16\"}, optimizer_kwargs = { \"eps\": 1e-7} , \n",
    "                                 learning_rate = 1e-3, batch_size=batch_size ,checkpoint_every_epoch = 25) \n",
    "loss = trainer.train(35, stop_at_loss = 0.00005)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be998d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "from meshgpt_pytorch import mesh_render \n",
    "\n",
    "min_mse, max_mse = float('inf'), float('-inf')\n",
    "min_coords, min_orgs, max_coords, max_orgs = None, None, None, None\n",
    "random_samples, random_samples_pred, all_random_samples = [], [], []\n",
    "total_mse, sample_size = 0.0, 200\n",
    "\n",
    "random.shuffle(dataset.data)\n",
    "autoencoder = autoencoder.to(\"cuda\")\n",
    "for item in tqdm(dataset.data[:sample_size]):\n",
    "    item['faces'] = item['faces'].to(\"cuda\")\n",
    "    item['vertices'] = item['vertices'].to(\"cuda\")\n",
    "    item['face_edges'] = item['face_edges'].to(\"cuda\")\n",
    "    codes = autoencoder.tokenize(vertices=item['vertices'], faces=item['faces'], face_edges=item['face_edges']) \n",
    "\n",
    "    codes = codes.flatten().unsqueeze(0)\n",
    "    codes = codes[:, :codes.shape[-1] // autoencoder.num_quantizers * autoencoder.num_quantizers] \n",
    " \n",
    "    coords, mask = autoencoder.decode_from_codes_to_faces(codes)\n",
    "    orgs = item['vertices'][item['faces']].unsqueeze(0)\n",
    "\n",
    "    mse = torch.mean((orgs.view(-1, 3).cpu() - coords.view(-1, 3).cpu())**2)\n",
    "    total_mse += mse \n",
    "\n",
    "    if mse < min_mse: min_mse, min_coords, min_orgs = mse, coords, orgs\n",
    "    if mse > max_mse: max_mse, max_coords, max_orgs = mse, coords, orgs\n",
    " \n",
    "    if len(random_samples) <= 30:\n",
    "        random_samples.append((coords, mask)) \n",
    "    else:\n",
    "        all_random_samples.extend([ random_samples])\n",
    "        random_samples, random_samples_pred = [], []\n",
    "\n",
    "print(f'MSE AVG: {total_mse / sample_size:.10f}, Min: {min_mse:.10f}, Max: {max_mse:.10f}')\n",
    "mesh_render.save_rendering(f'.\\mse_rows.obj', all_random_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "now_utc = datetime.datetime.now(datetime.timezone.utc).isoformat()\n",
    "now_safe = now_utc.replace(\":\", \"_\")\n",
    "filename = f\"./MeshGPT-autoencoder_{now_safe}.pt\"\n",
    "pkg = dict( model = autoencoder.state_dict(), ) \n",
    "torch.save(pkg, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
