{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /root/mesh-transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-06T16:31:09.323479Z",
          "iopub.status.busy": "2024-06-06T16:31:09.322599Z",
          "iopub.status.idle": "2024-06-06T16:31:30.718560Z",
          "shell.execute_reply": "2024-06-06T16:31:30.717443Z",
          "shell.execute_reply.started": "2024-06-06T16:31:09.323408Z"
        },
        "id": "LO7qIBFEGf4_",
        "outputId": "9c5eecc0-2d6a-44c0-efa2-a24d672c9ee7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "%pip install git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n",
        "%pip install matplotlib\n",
        "from pathlib import Path\n",
        "import gc\n",
        "import torch\n",
        "import os\n",
        "import torch\n",
        "from meshgpt_pytorch import (\n",
        "    MeshTransformerTrainer,\n",
        "    MeshAutoencoderTrainer,\n",
        "    MeshAutoencoder,\n",
        "    MeshTransformer,MeshDataset\n",
        ")\n",
        "from meshgpt_pytorch.data import (\n",
        "    derive_face_edges_from_faces\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0C6O6JM6Gf5A"
      },
      "outputs": [],
      "source": [
        "\n",
        "pkg = torch.load(\"./16k_autoencoder_229M_0.338.pt\", map_location=torch.device(\"cuda\"))\n",
        "autoencoder = MeshAutoencoder(\n",
        "        decoder_dims_through_depth =  (128,) * 6 + (192,) * 12 + (256,) * 24 + (384,) * 6,\n",
        "        dim_codebook = 192,\n",
        "        dim_area_embed = 16,\n",
        "        dim_coor_embed = 16,\n",
        "        dim_normal_embed = 16,\n",
        "        dim_angle_embed = 8,\n",
        "\n",
        "    attn_decoder_depth  = 4,\n",
        "    attn_encoder_depth = 2\n",
        "    ).to(\"cuda\")\n",
        "autoencoder.load_state_dict(pkg['model'])\n",
        "\n",
        "transformer = MeshTransformer(\n",
        "    autoencoder,\n",
        "    dim = 768,\n",
        "    coarse_pre_gateloop_depth =2,\n",
        "    fine_pre_gateloop_depth= 2,\n",
        "    attn_depth = 12,\n",
        "    attn_heads = 12,\n",
        "    fine_cross_attend_text = True,\n",
        "    text_cond_with_film = False,\n",
        "    cross_attn_num_mem_kv = 4,\n",
        "    num_sos_tokens = 1,\n",
        "    dropout  = 0.0,\n",
        "    max_seq_len = 1500,\n",
        "    fine_attn_depth = 2,\n",
        "    condition_on_text = True,\n",
        "    gateloop_use_heinsen = False,\n",
        "    text_condition_model_types = \"bge\",\n",
        "    text_condition_cond_drop_prob = 0.0,\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "livw0YyKdNux",
        "outputId": "7dc7cc81-9504-407f-8a35-a0a3fa0ef054"
      },
      "outputs": [],
      "source": [
        "# pkg = torch.load(\"./mesh-transformer.ckpt.epoch_0_avg_loss_8.859.pt\")\n",
        "# transformer.load_state_dict(pkg['model'], strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCFiJK24Gf5C",
        "outputId": "42845660-7969-4c49-cc67-373c6189eaf4"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "def train_mesh_transformer(dataset):\n",
        "    dataset.sort_dataset_keys()\n",
        "    batch_size = 16\n",
        "    grad_accum_every = 4\n",
        "    rate = 1e-2\n",
        "    trainer = MeshTransformerTrainer(model=transformer, warmup_steps=10, grad_accum_every=grad_accum_every,\n",
        "        num_train_steps=100, dataset=dataset, batch_size=batch_size, learning_rate=rate, checkpoint_every_epoch=1)\n",
        "    EARLY_STOP_LOSS = 0.00005\n",
        "    optimizer = optim.Adam(trainer.model.parameters(), lr=rate)\n",
        "    for epoch in range(100):\n",
        "        loss = trainer.train(1, stop_at_loss = EARLY_STOP_LOSS)\n",
        "        if loss <= EARLY_STOP_LOSS:\n",
        "            break\n",
        "    pkg = dict( model = transformer.state_dict(), )\n",
        "    torch.save(pkg, str(\"./MeshGPT-transformer_trained.pt\"))\n",
        "\n",
        "# the devops manual https://dl.acm.org/doi/book/10.5555/1869904 recommends making a build take 10 minutes as the maximum people can endure\n",
        "# currently labels_885_10x5_21720_mod.npz is sized for 10 minutes on a a100 40gb conviently.\n",
        "starting_datasets = [\n",
        "    \"/content/drive/Shareddrives/93 Text to Mesh - Ernest & Marcus/20240612_01/datasets/labels_885_10x5_21720_mod.npz\",\n",
        "]\n",
        "def process_dataset(datasets):\n",
        "    dataset = None\n",
        "    for ds in starting_datasets:\n",
        "        if dataset is None:\n",
        "            dataset = MeshDataset.load(ds)\n",
        "        else:\n",
        "            temp_dataset = MeshDataset.load(ds)\n",
        "            dataset.data.extend(temp_dataset.data)\n",
        "    return dataset\n",
        "\n",
        "train_mesh_transformer(process_dataset(starting_datasets))\n",
        "additional_datasets = [\n",
        "    \"./datasets/objverse_250f_45.9M_3086_labels_53730_10_min_x1_aug.npz\",\n",
        "    \"./datasets/t/objverse_250f_229.7M_3086_labels_268650_10_min_x5_aug.npz\"\n",
        "]\n",
        "train_mesh_transformer(process_dataset(additional_datasets))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5157660,
          "sourceId": 8617083,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5157661,
          "sourceId": 8617085,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30733,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
