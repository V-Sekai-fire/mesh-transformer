{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T16:31:09.323479Z","iopub.status.busy":"2024-06-06T16:31:09.322599Z","iopub.status.idle":"2024-06-06T16:31:30.718560Z","shell.execute_reply":"2024-06-06T16:31:30.717443Z","shell.execute_reply.started":"2024-06-06T16:31:09.323408Z"},"trusted":true},"outputs":[],"source":["%pip install git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n","%pip install matplotlib\n","from pathlib import Path \n","import gc    \n","import torch\n","import os\n","import torch  \n","from meshgpt_pytorch import (\n","    MeshTransformerTrainer,\n","    MeshAutoencoderTrainer,\n","    MeshAutoencoder,\n","    MeshTransformer,MeshDataset\n",")\n","from meshgpt_pytorch.data import ( \n","    derive_face_edges_from_faces\n",")   \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\ernest.lee\\scoop\\apps\\python\\current\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["\n","pkg = torch.load(\"./16k_autoencoder_229M_0.338.pt\", map_location=torch.device(\"cuda\")) \n","autoencoder = MeshAutoencoder( \n","        decoder_dims_through_depth =  (128,) * 6 + (192,) * 12 + (256,) * 24 + (384,) * 6,    \n","        dim_codebook = 192,  \n","        dim_area_embed = 16,\n","        dim_coor_embed = 16, \n","        dim_normal_embed = 16,\n","        dim_angle_embed = 8,\n","    \n","    attn_decoder_depth  = 4,\n","    attn_encoder_depth = 2\n","    ).to(\"cuda\")\n","autoencoder.load_state_dict(pkg['model'])\n","\n","transformer = MeshTransformer(\n","    autoencoder,\n","    dim = 768,\n","    coarse_pre_gateloop_depth =2,  \n","    fine_pre_gateloop_depth= 2, \n","    attn_depth = 12,  \n","    attn_heads = 12, \n","    fine_cross_attend_text = True,\n","    text_cond_with_film = False,\n","    cross_attn_num_mem_kv = 4,\n","    num_sos_tokens = 1, \n","    dropout  = 0.0,\n","    max_seq_len = 1500, \n","    fine_attn_depth = 2,\n","    condition_on_text = True, \n","    gateloop_use_heinsen = False,\n","    text_condition_model_types = \"bge\", \n","    text_condition_cond_drop_prob = 0.0, \n",").to(\"cuda\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["C:\\Users\\ernest.lee\\OneDrive\\Desktop\\text-to-mesh\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\ernest.lee\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n","  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"]}],"source":["%cd C:/Users/ernest.lee/OneDrive/Desktop/text-to-mesh"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[MeshDataset] Loaded 15170 entrys\n","[MeshDataset] Created from 15170 entrys\n","Last rate [0.01]\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/1:   0%|          | 0/948 [00:00<?, ?it/s]c:\\Users\\ernest.lee\\scoop\\apps\\python\\current\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n","  attn_output = torch.nn.functional.scaled_dot_product_attention(\n","c:\\Users\\ernest.lee\\scoop\\apps\\python\\current\\Lib\\site-packages\\torch\\backends\\cuda\\__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n","  warnings.warn(\n","Epoch 1/1:   4%|▍         | 37/948 [01:29<36:42,  2.42s/it, loss=9.78]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m             dataset\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mextend(temp_dataset\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[1;32m---> 40\u001b[0m \u001b[43mtrain_mesh_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstarting_datasets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m additional_datasets \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjverse_250f_45.9M_3086_labels_53730_10_min_x1_aug.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjverse_250f_229.7M_3086_labels_268650_10_min_x5_aug.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     44\u001b[0m ]\n\u001b[0;32m     45\u001b[0m train_mesh_transformer(process_dataset(additional_datasets))\n","Cell \u001b[1;32mIn[4], line 16\u001b[0m, in \u001b[0;36mtrain_mesh_transformer\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast rate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheduler\u001b[38;5;241m.\u001b[39mget_last_lr()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_at_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEARLY_STOP_LOSS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m EARLY_STOP_LOSS:\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\ernest.lee\\scoop\\apps\\python\\current\\Lib\\site-packages\\meshgpt_pytorch\\trainer.py:680\u001b[0m, in \u001b[0;36mMeshTransformerTrainer.train\u001b[1;34m(self, num_epochs, stop_at_loss, diplay_graph)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mautocast(), maybe_no_sync(): \n\u001b[0;32m    679\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch) \n\u001b[1;32m--> 680\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_accum_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m current_loss \u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    683\u001b[0m total_epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m current_loss  \n","File \u001b[1;32mc:\\Users\\ernest.lee\\scoop\\apps\\python\\current\\Lib\\site-packages\\accelerate\\accelerator.py:2134\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2134\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\ernest.lee\\scoop\\apps\\python\\current\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\ernest.lee\\scoop\\apps\\python\\current\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\ernest.lee\\scoop\\apps\\python\\current\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch.optim as optim\n","from torch.optim import lr_scheduler\n","\n","%env USE_FLASH_ATTENTION=1\n","\n","def train_mesh_transformer(dataset):\n","    dataset.sort_dataset_keys()\n","    batch_size = 16\n","    grad_accum_every = 4\n","    rate = 1e-2\n","    trainer = MeshTransformerTrainer(model=transformer, warmup_steps=10, grad_accum_every=grad_accum_every,\n","        num_train_steps=100, dataset=dataset, batch_size=batch_size, learning_rate=rate, checkpoint_every_epoch=1)\n","    EARLY_STOP_LOSS = 0.35\n","    optimizer = optim.Adam(trainer.model.parameters(), lr=rate)\n","    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","    for epoch in range(100):\n","        print(f\"Last rate {scheduler.get_last_lr()}\")\n","        loss = trainer.train(1, stop_at_loss = EARLY_STOP_LOSS)\n","        if loss <= EARLY_STOP_LOSS:\n","            break\n","        scheduler.step(loss) \n","        for param_group in optimizer.param_groups:\n","            rate = param_group['lr']\n","        trainer = MeshTransformerTrainer(model=transformer, warmup_steps=10, grad_accum_every=grad_accum_every,\n","                                        num_train_steps=100, dataset=dataset, learning_rate=rate, batch_size=batch_size, checkpoint_every_epoch=1)\n","    pkg = dict( model = transformer.state_dict(), ) \n","    torch.save(pkg, str(\"./MeshGPT-transformer_trained.pt\"))\n","    \n","starting_datasets = [\n","    \"labels_885_10x5_21720_mod.npz\",\n","]\n","def process_dataset(datasets):\n","    dataset = None\n","    for ds in starting_datasets:\n","        if dataset is None:\n","            dataset = MeshDataset.load(ds)\n","        else:\n","            temp_dataset = MeshDataset.load(ds)\n","            dataset.data.extend(temp_dataset.data)\n","    return dataset\n","\n","train_mesh_transformer(process_dataset(starting_datasets))\n","additional_datasets = [\n","    \"objverse_250f_45.9M_3086_labels_53730_10_min_x1_aug.npz\",\n","    \"objverse_250f_229.7M_3086_labels_268650_10_min_x5_aug.npz\"\n","]\n","train_mesh_transformer(process_dataset(additional_datasets))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5157660,"sourceId":8617083,"sourceType":"datasetVersion"},{"datasetId":5157661,"sourceId":8617085,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
