{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca92161",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n",
    "%pip install matplotlib\n",
    "\n",
    "%cd /root/text_to_mesh\n",
    "\n",
    "from pathlib import Path \n",
    "import gc    \n",
    "import torch\n",
    "import os\n",
    "import torch  \n",
    "from meshgpt_pytorch import (\n",
    "    MeshTransformerTrainer,\n",
    "    MeshAutoencoderTrainer,\n",
    "    MeshAutoencoder,\n",
    "    MeshTransformer,MeshDataset\n",
    ")\n",
    "from meshgpt_pytorch.data import ( \n",
    "    derive_face_edges_from_faces\n",
    ")\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37770c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 54\u001b[0m\n\u001b[0;32m     50\u001b[0m grad_accum_every \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m     \n\u001b[0;32m     51\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MeshTransformerTrainer(model \u001b[38;5;241m=\u001b[39m transformer,warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,grad_accum_every\u001b[38;5;241m=\u001b[39mgrad_accum_every,num_train_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, dataset \u001b[38;5;241m=\u001b[39m dataset, \n\u001b[0;32m     52\u001b[0m                                   accelerator_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed_precision\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m}, optimizer_kwargs \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1e-7\u001b[39m} , \n\u001b[0;32m     53\u001b[0m                                  learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size ,checkpoint_every_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m) \n\u001b[1;32m---> 54\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_at_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00005\u001b[39;49m\u001b[43m)\u001b[49m   \n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# pkg = torch.load(\"./MeshGPT-transformer_trained_base.pt\") \u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# transformer.load_state_dict(pkg['model'],strict=False)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ernest.lee\\scoop\\apps\\python\\current\\Lib\\site-packages\\meshgpt_pytorch\\trainer.py:680\u001b[0m, in \u001b[0;36mMeshTransformerTrainer.train\u001b[1;34m(self, num_epochs, stop_at_loss, diplay_graph)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mautocast(), maybe_no_sync(): \n\u001b[0;32m    679\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch) \n\u001b[1;32m--> 680\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_accum_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m current_loss \u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    683\u001b[0m total_epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m current_loss  \n",
      "File \u001b[1;32mc:\\Users\\ernest.lee\\scoop\\apps\\python\\current\\Lib\\site-packages\\accelerate\\accelerator.py:2130\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2130\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\ernest.lee\\scoop\\apps\\python\\current\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ernest.lee\\scoop\\apps\\python\\current\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ernest.lee\\scoop\\apps\\python\\current\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder = MeshAutoencoder( \n",
    "        decoder_dims_through_depth =  (128,) * 6 + (192,) * 12 + (256,) * 24 + (384,) * 6,    \n",
    "        dim_codebook = 192,  \n",
    "        dim_area_embed = 16,\n",
    "        dim_coor_embed = 16, \n",
    "        dim_normal_embed = 16,\n",
    "        dim_angle_embed = 8,\n",
    "    \n",
    "    attn_decoder_depth  = 4,\n",
    "    attn_encoder_depth = 2\n",
    "    ).to(\"cuda\")      \n",
    "\n",
    "pkg = torch.load(\"./16k_autoencoder_229M_0.338.pt\") \n",
    "autoencoder.load_state_dict(pkg['model'])\n",
    "for param in autoencoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "dataset = MeshDataset.load(\"./labels_885_10x5_21720_mod.npz\") \n",
    "labels = list(set(item['texts'] for item in dataset.data))\n",
    "dataset.generate_codes(autoencoder,150)\n",
    "dataset.data[0].keys() \n",
    "\n",
    "transformer = MeshTransformer(\n",
    "    autoencoder,\n",
    "    dim = 768,\n",
    "    coarse_pre_gateloop_depth =2,  \n",
    "    fine_pre_gateloop_depth= 2, \n",
    "    attn_depth = 12,  \n",
    "    attn_heads = 12, \n",
    "    fine_cross_attend_text = True,\n",
    "    text_cond_with_film = False,\n",
    "    cross_attn_num_mem_kv = 4,\n",
    "    num_sos_tokens = 1, \n",
    "    dropout  = 0.0,\n",
    "    max_seq_len = 1500, \n",
    "    fine_attn_depth = 2,\n",
    "    condition_on_text = True, \n",
    "    gateloop_use_heinsen = False,\n",
    "    text_condition_model_types = \"bge\", \n",
    "    text_condition_cond_drop_prob = 0.25, \n",
    ").to(\"cuda\")\n",
    "\n",
    "dataset.embed_texts(transformer,1)\n",
    "\n",
    "batch_size = 2\n",
    "grad_accum_every =4     \n",
    "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=grad_accum_every,num_train_steps=100, dataset = dataset, \n",
    "                                  accelerator_kwargs = {\"mixed_precision\" : \"fp16\"}, optimizer_kwargs = { \"eps\": 1e-7} , \n",
    "                                 learning_rate = 1e-3, batch_size=batch_size ,checkpoint_every_epoch = 25) \n",
    "loss = trainer.train(35, stop_at_loss = 0.00005)   \n",
    " \n",
    "# pkg = torch.load(\"./MeshGPT-transformer_trained_base.pt\") \n",
    "# transformer.load_state_dict(pkg['model'],strict=False)\n",
    "\n",
    "batch_size =16\n",
    "grad_accum_every =4      \n",
    "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=grad_accum_every,num_train_steps=100, dataset = dataset, \n",
    "                                 learning_rate = 1e-4, batch_size=batch_size, checkpoint_every_epoch = 1) \n",
    "\n",
    "total_epochs = 740\n",
    "epochs_per_save = 25\n",
    "\n",
    "for i in range(epochs_per_save, total_epochs + 1, epochs_per_save):\n",
    "    loss = trainer.train(i, stop_at_loss = 0.00005)   \n",
    "    pkg = dict( model = transformer.state_dict(), ) \n",
    "    now_utc = datetime.datetime.now(datetime.timezone.utc).isoformat().replace(\":\", \"_\").replace(\"+\", \"_\")\n",
    "    torch.save(pkg, str(f\"./MeshGPT-transformer_trained_{now_utc}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ff497",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = MeshAutoencoder( \n",
    "        decoder_dims_through_depth =  (128,) * 6 + (192,) * 12 + (256,) * 24 + (384,) * 6,   \n",
    "        codebook_size =  2048, \n",
    "        dim_codebook = 192,  \n",
    "        dim_area_embed = 16,\n",
    "        dim_coor_embed = 16, \n",
    "        dim_normal_embed = 16,\n",
    "        dim_angle_embed = 8,\n",
    "    \n",
    "    attn_decoder_depth  = 4,\n",
    "    attn_encoder_depth = 2\n",
    "    ).to(\"cuda\")     \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()  \n",
    " \n",
    "transformer = MeshTransformer(\n",
    "    autoencoder,\n",
    "    dim = 768,\n",
    "    coarse_pre_gateloop_depth =2,  \n",
    "    fine_pre_gateloop_depth= 2, \n",
    "    attn_depth = 12,  \n",
    "    attn_heads = 12, \n",
    "    fine_cross_attend_text = True,\n",
    "    text_cond_with_film = False,\n",
    "    cross_attn_num_mem_kv = 4,\n",
    "    num_sos_tokens = 1, \n",
    "    dropout  = 0.0,\n",
    "    max_seq_len = 1500, \n",
    "    fine_attn_depth = 2,\n",
    "    condition_on_text = True, \n",
    "    gateloop_use_heinsen = False,\n",
    "    text_condition_model_types = \"bge\", \n",
    "    text_condition_cond_drop_prob = 0.25, \n",
    ").to(\"cuda\")\n",
    "\n",
    "pkg = torch.load(\"./MeshGPT-transformer_trained_base.pt\") \n",
    "transformer.load_state_dict(pkg['model'],strict=False)\n",
    " \n",
    "dataset = MeshDataset.load(\"./labels_885_10x5_21720_mod.npz\") \n",
    "labels = list(set(item['texts'] for item in dataset.data))\n",
    "dataset.generate_codes(autoencoder,150)\n",
    "dataset.data[0].keys() \n",
    "dataset.embed_texts(transformer,1)\n",
    "\n",
    "batch_size =16\n",
    "grad_accum_every =4      \n",
    "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=grad_accum_every,num_train_steps=100, dataset = dataset, \n",
    "                                 learning_rate = 1e-4, batch_size=batch_size, checkpoint_every_epoch = 25) \n",
    "\n",
    "total_epochs = 740\n",
    "epochs_per_save = 25\n",
    "\n",
    "for i in range(epochs_per_save, total_epochs + 1, epochs_per_save):\n",
    "    loss = trainer.train(i, stop_at_loss = 0.00005)   \n",
    "    pkg = dict( model = transformer.state_dict(), ) \n",
    "    now_utc = datetime.datetime.now(datetime.timezone.utc).isoformat().replace(\":\", \"_\") \n",
    "    torch.save(pkg, str(f\"./MeshGPT-transformer_trained_{now_utc}.pt\"))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
