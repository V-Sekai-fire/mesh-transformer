{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-06T16:31:09.323479Z","iopub.status.busy":"2024-06-06T16:31:09.322599Z","iopub.status.idle":"2024-06-06T16:31:30.718560Z","shell.execute_reply":"2024-06-06T16:31:30.717443Z","shell.execute_reply.started":"2024-06-06T16:31:09.323408Z"},"trusted":true},"outputs":[],"source":["%pip install git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n","%pip install matplotlib\n","from pathlib import Path \n","import gc    \n","import torch\n","import os\n","import torch  \n","from meshgpt_pytorch import (\n","    MeshTransformerTrainer,\n","    MeshAutoencoderTrainer,\n","    MeshAutoencoder,\n","    MeshTransformer,MeshDataset\n",")\n","from meshgpt_pytorch.data import ( \n","    derive_face_edges_from_faces\n",")   \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\ernest.lee\\scoop\\apps\\python\\current\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["\n","pkg = torch.load(\"./16k_autoencoder_229M_0.338.pt\", map_location=torch.device(\"cuda\")) \n","autoencoder = MeshAutoencoder( \n","        decoder_dims_through_depth =  (128,) * 6 + (192,) * 12 + (256,) * 24 + (384,) * 6,    \n","        dim_codebook = 192,  \n","        dim_area_embed = 16,\n","        dim_coor_embed = 16, \n","        dim_normal_embed = 16,\n","        dim_angle_embed = 8,\n","    \n","    attn_decoder_depth  = 4,\n","    attn_encoder_depth = 2\n","    ).to(\"cuda\")\n","autoencoder.load_state_dict(pkg['model'])\n","\n","transformer = MeshTransformer(\n","    autoencoder,\n","    dim = 768,\n","    coarse_pre_gateloop_depth =2,  \n","    fine_pre_gateloop_depth= 2, \n","    attn_depth = 12,  \n","    attn_heads = 12, \n","    fine_cross_attend_text = True,\n","    text_cond_with_film = False,\n","    cross_attn_num_mem_kv = 4,\n","    num_sos_tokens = 1, \n","    dropout  = 0.0,\n","    max_seq_len = 1500, \n","    fine_attn_depth = 2,\n","    condition_on_text = True, \n","    gateloop_use_heinsen = False,\n","    text_condition_model_types = \"bge\", \n","    text_condition_cond_drop_prob = 0.0, \n",").to(\"cuda\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["C:\\Users\\ernest.lee\\OneDrive\\Desktop\\text-to-mesh\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\ernest.lee\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n","  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"]}],"source":["%cd C:/Users/ernest.lee/OneDrive/Desktop/text-to-mesh"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[MeshDataset] Loaded 15170 entrys\n","[MeshDataset] Created from 15170 entrys\n"]},{"ename":"AttributeError","evalue":"'list' object has no attribute 'data'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[8], line 37\u001b[0m\n\u001b[0;32m     29\u001b[0m         trainer \u001b[38;5;241m=\u001b[39m MeshTransformerTrainer(model\u001b[38;5;241m=\u001b[39mtransformer, warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, grad_accum_every\u001b[38;5;241m=\u001b[39mgrad_accum_every,\n\u001b[0;32m     30\u001b[0m                                         num_train_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, dataset\u001b[38;5;241m=\u001b[39mdataset, learning_rate\u001b[38;5;241m=\u001b[39mrate, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, checkpoint_every_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m starting_datasets \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels_885_10x5_21720_mod.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m ]\n\u001b[1;32m---> 37\u001b[0m \u001b[43mtrain_mesh_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstarting_datasets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m additional_datasets \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjverse_250f_45.9M_3086_labels_53730_10_min_x1_aug.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjverse_250f_229.7M_3086_labels_268650_10_min_x5_aug.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m ]\n\u001b[0;32m     42\u001b[0m train_mesh_transformer(additional_datasets)\n","Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36mtrain_mesh_transformer\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m         temp_dataset \u001b[38;5;241m=\u001b[39m MeshDataset\u001b[38;5;241m.\u001b[39mload(ds)\n\u001b[1;32m---> 10\u001b[0m         \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mextend(temp_dataset\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m     12\u001b[0m dataset\u001b[38;5;241m.\u001b[39msort_dataset_keys()\n\u001b[0;32m     13\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n","\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'data'"]}],"source":["import torch.optim as optim\n","from torch.optim import lr_scheduler\n","\n","def train_mesh_transformer(datasets):\n","    for ds in datasets:\n","        if dataset is None:\n","            dataset = MeshDataset.load(ds)\n","        else:\n","            temp_dataset = MeshDataset.load(ds)\n","            dataset.data.extend(temp_dataset.data)\n","\n","    dataset.sort_dataset_keys()\n","    batch_size = 16\n","    grad_accum_every = 4\n","    rate = 1e-2\n","    trainer = MeshTransformerTrainer(model=transformer, warmup_steps=10, grad_accum_every=grad_accum_every,\n","        num_train_steps=100, dataset=dataset, batch_size=batch_size, learning_rate=rate, checkpoint_every_epoch=1)\n","    EARLY_STOP_LOSS = 0.35\n","    optimizer = optim.Adam(trainer.model.parameters(), lr=rate)\n","    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","    for epoch in range(100):\n","        print(f\"Last rate {scheduler.get_last_lr()}\")\n","        loss = trainer.train(1, stop_at_loss = EARLY_STOP_LOSS)\n","        if loss <= EARLY_STOP_LOSS:\n","            break\n","        scheduler.step(loss) \n","        for param_group in optimizer.param_groups:\n","            rate = param_group['lr']\n","        trainer = MeshTransformerTrainer(model=transformer, warmup_steps=10, grad_accum_every=grad_accum_every,\n","                                        num_train_steps=100, dataset=dataset, learning_rate=rate, batch_size=batch_size, checkpoint_every_epoch=1)\n","        \n","\n","starting_datasets = [\n","    \"labels_885_10x5_21720_mod.npz\",\n","]\n","\n","train_mesh_transformer(starting_datasets)\n","additional_datasets = [\n","    \"objverse_250f_45.9M_3086_labels_53730_10_min_x1_aug.npz\",\n","    \"objverse_250f_229.7M_3086_labels_268650_10_min_x5_aug.npz\"\n","]\n","train_mesh_transformer(additional_datasets)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pkg = dict( model = transformer.state_dict(), ) \n","torch.save(pkg, str(\"./MeshGPT-transformer_trained.pt\"))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5157660,"sourceId":8617083,"sourceType":"datasetVersion"},{"datasetId":5157661,"sourceId":8617085,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
